{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Análisis de Pensiones Colombianas\n",
        "\n",
        "Notebook guía que documenta el pipeline completo de descarga, limpieza, enriquecimiento y modelado del dataset `uawh-cjvi` publicado en Datos Abiertos Colombia.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Ruta de trabajo\n",
        "\n",
        "1. Conectar con la API de Socrata y descargar la totalidad del recurso.\n",
        "2. Estandarizar tipos de datos, controlar calidad y generar diccionarios de apoyo.\n",
        "3. Deduplicar, detectar outliers y enriquecer el dataset con variables derivadas.\n",
        "4. Exportar productos de datos (dataset limpio, resumen de ejecución y subconjuntos).\n",
        "5. Construir un modelo base con árbol de decisión para explorar la relevancia de variables.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "import time\n",
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import requests\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from scipy.stats import entropy as shannon_entropy\n",
        "\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.feature_selection import mutual_info_classif\n",
        "from sklearn.inspection import permutation_importance\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
        "\n",
        "pd.set_option(\"display.max_columns\", None)\n",
        "sns.set_theme(style=\"whitegrid\")\n",
        "\n",
        "BASE = \"https://www.datos.gov.co\"\n",
        "RESOURCE = \"uawh-cjvi\"\n",
        "URL = f\"{BASE}/resource/{RESOURCE}.json\"\n",
        "\n",
        "RAW_DIR = Path(\"data/raw\")\n",
        "PROCESSED_DIR = Path(\"data/processed\")\n",
        "REPORTS_DIR = Path(\"data/reports\")\n",
        "for path in (RAW_DIR, PROCESSED_DIR, REPORTS_DIR):\n",
        "    path.mkdir(parents=True, exist_ok=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Descarga completa desde la API\n",
        "\n",
        "Socrata limita las respuestas a 1000 filas; se implementa paginación con `$limit` y `$offset` para traer todo el histórico.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "def fetch_full_dataset(limit=50_000, delay=0.3):\n",
        "    pages = []\n",
        "    offset = 0\n",
        "    while True:\n",
        "        params = {\"$limit\": limit, \"$offset\": offset}\n",
        "        response = requests.get(URL, params=params, timeout=120)\n",
        "        response.raise_for_status()\n",
        "        chunk = response.json()\n",
        "        if not chunk:\n",
        "            break\n",
        "        pages.append(pd.DataFrame(chunk))\n",
        "        offset += limit\n",
        "        print(f\"Descargadas: {offset} filas…\")\n",
        "        time.sleep(delay)\n",
        "    if pages:\n",
        "        return pd.concat(pages, ignore_index=True)\n",
        "    return pd.DataFrame()\n",
        "\n",
        "try:\n",
        "    total_filas = int(requests.get(f\"{URL}?$select=count(*)\").json()[0][\"count\"])\n",
        "except Exception:\n",
        "    total_filas = None\n",
        "\n",
        "print(f\"Total reportado por la API: {total_filas}\")\n",
        "df_raw = fetch_full_dataset()\n",
        "print(f\"Filas descargadas: {len(df_raw)}\")\n",
        "\n",
        "df = df_raw.copy()\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exploración inicial\n",
        "\n",
        "Revisamos la estructura del DataFrame y una muestra de filas para validar la descarga.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "if not df.empty:\n",
        "    display(df.head())\n",
        "    display(df.sample(min(5, len(df)), random_state=42))\n",
        "df.info()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Limpieza de tipos\n",
        "\n",
        "Se fuerza `fecha` a formato `datetime` y `valor_unidad` a `float`, eliminando símbolos indeseados.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "df[\"fecha\"] = pd.to_datetime(df[\"fecha\"], errors=\"coerce\")\n",
        "df[\"valor_unidad\"] = (\n",
        "    df[\"valor_unidad\"]\n",
        "    .astype(str)\n",
        "    .str.replace(r\"[^\\d\\-,\\.]\", \"\", regex=True)\n",
        "    .str.replace(\",\", \".\", regex=False)\n",
        "    .astype(float)\n",
        ")\n",
        "\n",
        "df.dtypes\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Calidad de datos\n",
        "\n",
        "Calculamos porcentaje de nulos, cardinalidades y verificamos la relación uno a uno entre códigos y etiquetas.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "nulls = df.isna().mean().sort_values(ascending=False).mul(100).round(2)\n",
        "cardinalidad = df.nunique(dropna=True).sort_values(ascending=False)\n",
        "\n",
        "print(\"% de nulos por columna:\")\n",
        "display(nulls)\n",
        "\n",
        "print(\"\n",
        "Cardinalidad por columna:\")\n",
        "display(cardinalidad)\n",
        "\n",
        "if {\"codigo_entidad\", \"nombre_entidad\"}.issubset(df.columns):\n",
        "    rel_entidad = df.groupby(\"codigo_entidad\")[\"nombre_entidad\"].nunique().sort_values(ascending=False).head()\n",
        "    print(\"\n",
        "Relación código_entidad → nombre_entidad:\")\n",
        "    display(rel_entidad)\n",
        "\n",
        "if {\"codigo_patrimonio\", \"nombre_fondo\"}.issubset(df.columns):\n",
        "    rel_fondo = df.groupby(\"codigo_patrimonio\")[\"nombre_fondo\"].nunique().sort_values(ascending=False).head()\n",
        "    print(\"\n",
        "Relación código_patrimonio → nombre_fondo:\")\n",
        "    display(rel_fondo)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Diccionarios de entidades y fondos\n",
        "\n",
        "Construimos diccionarios de referencia y los exportamos para consultas rápidas.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "dict_entidad = (\n",
        "    df[[\"nombre_entidad\", \"codigo_entidad\"]]\n",
        "    .drop_duplicates()\n",
        "    .dropna(subset=[\"nombre_entidad\", \"codigo_entidad\"])\n",
        "    .set_index(\"nombre_entidad\")[\"codigo_entidad\"]\n",
        "    .to_dict()\n",
        ")\n",
        "\n",
        "support = df[[\"nombre_fondo\", \"codigo_patrimonio\"]]\n",
        "support = support.drop_duplicates().dropna(subset=[\"nombre_fondo\", \"codigo_patrimonio\"])\n",
        "dict_fondo = support.set_index(\"nombre_fondo\")[\"codigo_patrimonio\"].to_dict()\n",
        "\n",
        "pd.DataFrame(list(dict_entidad.items()), columns=[\"nombre_entidad\", \"codigo_entidad\"]).to_csv(\n",
        "    RAW_DIR / \"entidad_codigo.csv\", index=False\n",
        ")\n",
        "pd.DataFrame(list(dict_fondo.items()), columns=[\"nombre_fondo\", \"codigo_patrimonio\"]).to_csv(\n",
        "    RAW_DIR / \"fondos_codigo.csv\", index=False\n",
        ")\n",
        "\n",
        "print(f\"Diccionario de entidades: {len(dict_entidad)} entradas\")\n",
        "print(f\"Diccionario de fondos: {len(dict_fondo)} entradas\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Normalización de texto\n",
        "\n",
        "Homogeneizamos espacios y mayúsculas/minúsculas en nombres de entidad y fondo para reducir cardinalidad artificial.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "for columna in [\"nombre_entidad\", \"nombre_fondo\"]:\n",
        "    if columna in df.columns:\n",
        "        df[columna] = (\n",
        "            df[columna]\n",
        "            .astype(str)\n",
        "            .str.strip()\n",
        "            .str.replace(r\"\\s+\", \" \", regex=True)\n",
        "        )\n",
        "\n",
        "df[[\"nombre_entidad\", \"nombre_fondo\"]].nunique()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Deduplicación\n",
        "\n",
        "Se eliminan duplicados exactos y, posteriormente, duplicados conceptuales definidos por (`nombre_entidad`, `nombre_fondo`, `fecha`).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "duplicados = df.duplicated().sum()\n",
        "print(f\"Filas duplicadas exactas: {duplicados}\")\n",
        "if duplicados > 0:\n",
        "    df = df.drop_duplicates()\n",
        "    print(f\"Dataset tras remover duplicados exactos: {len(df)} filas\")\n",
        "\n",
        "duplicados_conceptuales = df.duplicated(subset=[\"nombre_entidad\", \"nombre_fondo\", \"fecha\"]).sum()\n",
        "print(f\"Duplicados conceptuales: {duplicados_conceptuales}\")\n",
        "if duplicados_conceptuales > 0:\n",
        "    df = df.drop_duplicates(subset=[\"nombre_entidad\", \"nombre_fondo\", \"fecha\"], keep=\"first\")\n",
        "    print(f\"Dataset tras limpieza conceptual: {len(df)} filas\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Detección de outliers\n",
        "\n",
        "Utilizamos el rango intercuartílico (IQR) para detectar observaciones atípicas en `valor_unidad` y creamos una bandera `es_outlier`.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "Q1 = df['valor_unidad'].quantile(0.25)\n",
        "Q3 = df['valor_unidad'].quantile(0.75)\n",
        "IQR = Q3 - Q1\n",
        "limite_inferior = Q1 - 1.5 * IQR\n",
        "limite_superior = Q3 + 1.5 * IQR\n",
        "\n",
        "outliers = df[(df['valor_unidad'] < limite_inferior) | (df['valor_unidad'] > limite_superior)]\n",
        "porcentaje_outliers = (len(outliers) / len(df) * 100) if len(df) else 0\n",
        "\n",
        "df['es_outlier'] = (\n",
        "    (df['valor_unidad'] < limite_inferior) |\n",
        "    (df['valor_unidad'] > limite_superior)\n",
        ")\n",
        "\n",
        "print(f\"Límite inferior: {limite_inferior:.2f}\")\n",
        "print(f\"Límite superior: {limite_superior:.2f}\")\n",
        "print(f\"Outliers detectados: {len(outliers)} ({porcentaje_outliers:.2f}%)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Optimización y variables derivadas\n",
        "\n",
        "Convertimos columnas de alta repetición a categoría y generamos atributos temporales y de tipología de fondo.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "if 'nombre_entidad' in df.columns:\n",
        "    df['nombre_entidad'] = df['nombre_entidad'].astype('category')\n",
        "if 'nombre_fondo' in df.columns:\n",
        "    df['nombre_fondo'] = df['nombre_fondo'].astype('category')\n",
        "df['es_outlier'] = df['es_outlier'].astype('bool')\n",
        "\n",
        "df['año'] = df['fecha'].dt.year\n",
        "df['mes'] = df['fecha'].dt.month\n",
        "df['trimestre'] = df['fecha'].dt.quarter\n",
        "\n",
        "def clasificar_fondo(nombre_fondo: str) -> str:\n",
        "    nombre = str(nombre_fondo).lower()\n",
        "    if 'cesantia' in nombre:\n",
        "        return 'Cesantías'\n",
        "    if 'pension' in nombre:\n",
        "        return 'Pensiones'\n",
        "    if 'alternativo' in nombre:\n",
        "        return 'Alternativo'\n",
        "    return 'Otros'\n",
        "\n",
        "df['tipo_fondo'] = df['nombre_fondo'].astype(str).apply(clasificar_fondo).astype('category')\n",
        "\n",
        "df[['tipo_fondo', 'año', 'mes', 'trimestre']].head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exportaciones principales\n",
        "\n",
        "Generamos el dataset limpio, un resumen de la sesión y guardamos versiones sin códigos para análisis posteriores.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "columnas_a_eliminar = [c for c in [\"codigo_entidad\", \"codigo_patrimonio\"] if c in df.columns]\n",
        "df_clean = df.drop(columns=columnas_a_eliminar)\n",
        "\n",
        "df_clean.to_csv(RAW_DIR / \"pensionesLimpio.csv\", index=False)\n",
        "df.to_csv(PROCESSED_DIR / \"pensiones_limpio_final.csv\", index=False, encoding='utf-8')\n",
        "\n",
        "resumen_limpieza = {\n",
        "    'filas_finales': len(df),\n",
        "    'columnas_finales': len(df.columns),\n",
        "    'duplicados_eliminados': int(duplicados),\n",
        "    'duplicados_conceptuales_eliminados': int(duplicados_conceptuales),\n",
        "    'outliers_detectados': int(len(outliers)),\n",
        "    'memoria_mb': float(df.memory_usage(deep=True).sum() / 1024**2),\n",
        "    'fecha_limpieza': pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')\n",
        "}\n",
        "\n",
        "pd.Series(resumen_limpieza).to_csv(PROCESSED_DIR / \"resumen_limpieza.csv\")\n",
        "resumen_limpieza\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Subconjuntos temáticos\n",
        "\n",
        "Se generan CSV específicos por entidad y por tipo de fondo que facilitan análisis focalizados.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "def guardar_subset(df_origen, columna, valores, nombre_archivo, match=\"exact\"):\n",
        "    if isinstance(valores, (list, tuple, set)):\n",
        "        valores_iter = list(valores)\n",
        "    else:\n",
        "        valores_iter = [valores]\n",
        "\n",
        "    if match == \"contains\":\n",
        "        mascara = pd.Series(False, index=df_origen.index)\n",
        "        for valor in valores_iter:\n",
        "            mascara |= df_origen[columna].astype(str).str.contains(valor, case=False, na=False)\n",
        "    else:\n",
        "        mascara = df_origen[columna].isin(valores_iter)\n",
        "\n",
        "    subset = df_origen.loc[mascara].copy()\n",
        "    if columna in subset.columns:\n",
        "        subset = subset.drop(columns=[columna])\n",
        "    ruta = RAW_DIR / nombre_archivo\n",
        "    subset.to_csv(ruta, index=False)\n",
        "    print(f\"{ruta}: {subset.shape[0]} filas\")\n",
        "\n",
        "guardar_subset(df_clean, \"nombre_entidad\", [\"Skandia\", \"Skandia Afp - Accai\"], \"pensiones_skandia.csv\", match=\"contains\")\n",
        "guardar_subset(df_clean, \"nombre_entidad\", [\"Proteccion\"], \"pensiones_proteccion.csv\", match=\"contains\")\n",
        "guardar_subset(df_clean, \"nombre_entidad\", [\"Porvenir\"], \"pensiones_porvenir.csv\", match=\"contains\")\n",
        "guardar_subset(df_clean, \"nombre_entidad\", [\"Colfondos\"], \"colfondos_colfondos.csv\", match=\"contains\")\n",
        "\n",
        "guardar_subset(df_clean, \"nombre_fondo\", [\"Fondo de Cesantias Largo Plazo\"], \"fondo_cesantias_largo_plazo.csv\")\n",
        "guardar_subset(df_clean, \"nombre_fondo\", [\"Fondo de Cesantias Corto Plazo\"], \"fondo_cesantias_corto_plazo.csv\")\n",
        "guardar_subset(df_clean, \"nombre_fondo\", [\"Fondo de Pensiones Moderado\"], \"fondo_pensiones_moderado.csv\")\n",
        "guardar_subset(df_clean, \"nombre_fondo\", [\"Fondo de Pensiones Conservador\"], \"fondo_pensiones_conservador.csv\")\n",
        "guardar_subset(df_clean, \"nombre_fondo\", [\"Fondo de Pensiones Mayor Riesgo\"], \"fondo_pensiones_mayor_riesgo.csv\")\n",
        "guardar_subset(df_clean, \"nombre_fondo\", [\"Fondo de Pensiones Retiro Programado\"], \"fondo_pensiones_retiro_programado.csv\")\n",
        "guardar_subset(df_clean, \"nombre_fondo\", [\"Fondo de Pensiones Alternativo\"], \"fondo_pensiones_alternativo.csv\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Modelo exploratorio: árbol de decisión\n",
        "\n",
        "Se entrena un árbol de decisión poco profundo para estimar terciles de `valor_unidad` y estudiar la relevancia de los predictores disponibles.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "df_modelo = df_clean.dropna(subset=['valor_unidad']).copy()\n",
        "df_modelo['target_bin'] = pd.qcut(df_modelo['valor_unidad'], q=3, labels=['bajo', 'medio', 'alto'])\n",
        "\n",
        "num_cols = ['valor_unidad']\n",
        "cat_cols = [c for c in ['nombre_entidad', 'nombre_fondo', 'tipo_fondo'] if c in df_modelo.columns]\n",
        "\n",
        "X = df_modelo[num_cols + cat_cols].copy()\n",
        "y = df_modelo['target_bin'].astype('category')\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.25, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "print(f\"Train size: {X_train.shape}, Test size: {X_test.shape}\")\n",
        "\n",
        "def shannon_entropy_categorical(serie: pd.Series) -> float:\n",
        "    counts = serie.value_counts(dropna=True)\n",
        "    probabilidades = counts / counts.sum()\n",
        "    return float(shannon_entropy(probabilidades, base=2))\n",
        "\n",
        "def shannon_entropy_numeric(serie: pd.Series, bins: int = 20) -> float:\n",
        "    serie = serie.dropna()\n",
        "    hist, _ = np.histogram(serie, bins=bins)\n",
        "    probabilidades = hist / hist.sum() if hist.sum() else np.array([1.0])\n",
        "    return float(shannon_entropy(probabilidades, base=2))\n",
        "\n",
        "entropias = {}\n",
        "for col in cat_cols:\n",
        "    entropias[col] = shannon_entropy_categorical(X_train[col].astype(str))\n",
        "for col in num_cols:\n",
        "    entropias[col] = shannon_entropy_numeric(X_train[col].astype(float), bins=20)\n",
        "entropias = pd.Series(entropias).sort_values(ascending=False)\n",
        "\n",
        "print(\"Entropía (bits) por variable:\")\n",
        "display(entropias)\n",
        "\n",
        "preprocesador = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('cat', OneHotEncoder(handle_unknown='ignore', sparse=False), cat_cols),\n",
        "        ('num', 'passthrough', num_cols)\n",
        "    ],\n",
        "    remainder='drop'\n",
        ")\n",
        "\n",
        "X_train_enc = preprocesador.fit_transform(X_train)\n",
        "X_test_enc = preprocesador.transform(X_test)\n",
        "\n",
        "cat_feature_names = list(preprocesador.named_transformers_['cat'].get_feature_names_out(cat_cols)) if cat_cols else []\n",
        "feature_names = cat_feature_names + num_cols\n",
        "\n",
        "mi_scores = mutual_info_classif(X_train_enc, y_train.cat.codes, random_state=42)\n",
        "mi_series = pd.Series(mi_scores, index=feature_names).sort_values(ascending=False)\n",
        "print(\"Información mutua con la clase:\")\n",
        "display(mi_series.head(20))\n",
        "\n",
        "arbol = DecisionTreeClassifier(\n",
        "    criterion='entropy',\n",
        "    max_depth=3,\n",
        "    min_samples_leaf=50,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "pipeline = Pipeline([\n",
        "    ('prep', preprocesador),\n",
        "    ('tree', arbol)\n",
        "])\n",
        "pipeline.fit(X_train, y_train)\n",
        "y_pred = pipeline.predict(X_test)\n",
        "\n",
        "print(\"Reporte de clasificación:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "print(\"Matriz de confusión:\")\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "\n",
        "arbol_vis = DecisionTreeClassifier(\n",
        "    criterion='entropy',\n",
        "    max_depth=3,\n",
        "    min_samples_leaf=50,\n",
        "    random_state=42\n",
        ")\n",
        "arbol_vis.fit(X_train_enc, y_train)\n",
        "\n",
        "plt.figure(figsize=(18, 8))\n",
        "plot_tree(\n",
        "    arbol_vis,\n",
        "    feature_names=feature_names,\n",
        "    class_names=list(y_train.cat.categories),\n",
        "    filled=True,\n",
        "    rounded=True,\n",
        "    impurity=True\n",
        ")\n",
        "plt.title(\"Árbol de decisión (criterio = entropía)\")\n",
        "plt.show()\n",
        "\n",
        "importancias_arbol = pd.Series(arbol_vis.feature_importances_, index=feature_names)\n",
        "print(\"Importancias por reducción de entropía:\")\n",
        "display(importancias_arbol.sort_values(ascending=False).head(20))\n",
        "\n",
        "importancias_perm = permutation_importance(\n",
        "    pipeline,\n",
        "    X_test,\n",
        "    y_test,\n",
        "    n_repeats=10,\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "importancias_perm_df = pd.Series(importancias_perm.importances_mean, index=num_cols + cat_cols).sort_values(ascending=False)\n",
        "print(\"Importancias por permutación:\")\n",
        "display(importancias_perm_df.head(20))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exportes analíticos\n",
        "\n",
        "Guardamos los resultados del análisis de importancia de variables y el gráfico del árbol para documentar el modelo exploratorio.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "mi_series.head(20).to_csv(REPORTS_DIR / \"top20_info_mutua.csv\")\n",
        "importancias_arbol.sort_values(ascending=False).to_csv(REPORTS_DIR / \"importancias_arbol_entropy.csv\")\n",
        "importancias_perm_df.to_csv(REPORTS_DIR / \"importancias_permutacion.csv\")\n",
        "\n",
        "plt.figure(figsize=(18, 8))\n",
        "plot_tree(\n",
        "    arbol_vis,\n",
        "    feature_names=feature_names,\n",
        "    class_names=list(y_train.cat.categories),\n",
        "    filled=True,\n",
        "    rounded=True,\n",
        "    impurity=True\n",
        ")\n",
        "plt.title(\"Árbol de decisión (criterio = entropía)\")\n",
        "plt.savefig(REPORTS_DIR / \"arbol_decision_entropy.png\", dpi=300, bbox_inches='tight')\n",
        "plt.close()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Próximos pasos\n",
        "\n",
        "- Incorporar visualizaciones adicionales (series de tiempo, boxplots por tipo de fondo) sobre `df_clean`.\n",
        "- Ajustar hiperparámetros o probar modelos alternativos (p. ej. `RandomForestClassifier`) manteniendo la misma matriz de diseño.\n",
        "- Automatizar la actualización periódica del dataset mediante tareas programadas.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}