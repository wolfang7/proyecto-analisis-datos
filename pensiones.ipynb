{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f12dcc92",
   "metadata": {},
   "source": [
    "# Proyecto\n",
    "## 1. Extracción y análisis de la base de datos Colfondos\n",
    "\n",
    "En este notebook se documenta paso a paso la extracción, limpieza y análisis inicial \n",
    "del dataset público de Colfondos disponible en [datos.gov.co](https://www.datos.gov.co/).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bccd25f",
   "metadata": {},
   "source": [
    "## 2. Importamos las librerías que vamos a usar para análizar el dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfec591e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import time\n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "496b3d02",
   "metadata": {},
   "source": [
    "## 3. Conexión a la API\n",
    "\n",
    "El dataset se encuentra en la plataforma Socrata, y por defecto devuelve \n",
    "máximo 1000 filas. Para obtener todo el dataset, se usa paginación \n",
    "con los parámetros `$limit` y `$offset`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e45448d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE = \"https://www.datos.gov.co\"\n",
    "RESOURCE = \"uawh-cjvi\"\n",
    "URL = f\"{BASE}/resource/{RESOURCE}.json\"\n",
    "\n",
    "try:\n",
    "    total_filas = int(requests.get(f\"{URL}?$select=count(*)\").json()[0][\"count\"])\n",
    "except Exception:\n",
    "    total_filas  = None\n",
    "print(\"Total reportado:\", total_filas )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7d51fa9",
   "metadata": {},
   "source": [
    "### 4. Descarga de datos en páginas\n",
    "Usamos un bucle `while` para traer todas las filas de la base, \n",
    "concatenando los resultados en un único Diccionario.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1eb24f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Lista_paginas = []\n",
    "limit = 50000         \n",
    "offset = 0\n",
    "while True:\n",
    "    params = {\"$limit\": limit, \"$offset\": offset}\n",
    "    r = requests.get(URL, params=params, timeout=120)\n",
    "    r.raise_for_status()\n",
    "    respuestaJson = r.json()\n",
    "    if not respuestaJson: # fin de datos\n",
    "        break\n",
    "    Lista_paginas.append(pd.DataFrame(respuestaJson))\n",
    "    offset += limit\n",
    "    print(f\"Descargadas: {offset} filas…\")\n",
    "    time.sleep(0.3)     \n",
    "\n",
    "if Lista_paginas:  \n",
    "    df = pd.concat(Lista_paginas, ignore_index=True)\n",
    "else:              \n",
    "    df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1acc68de",
   "metadata": {},
   "source": [
    "## 5. Exploración inicial\n",
    "Mostramos las primeras filas y las columnas disponibles en el dataset.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2174af47",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()\n",
    "df.info()\n",
    "df.describe(include=\"all\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6141774",
   "metadata": {},
   "source": [
    "## 6. Limpieza de datos\n",
    "6.1. Convertimos las fechas al tipo `datetime` y el valor de la unidad a `float`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f9e51fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"fecha\"] = pd.to_datetime(df[\"fecha\"], errors=\"coerce\")\n",
    "df[\"valor_unidad\"] = (\n",
    "    df[\"valor_unidad\"]\n",
    "      .astype(str)\n",
    "      .str.replace(r\"[^\\d\\-,\\.]\", \"\", regex=True)\n",
    "      .str.replace(\",\", \".\", regex=False)\n",
    "      .astype(float)\n",
    ")\n",
    "df.dtypes\n",
    "df.to_csv(\"data/raw/pensionesLimpio.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0bead02",
   "metadata": {},
   "source": [
    "6.2. Vamos a ver si hay valores nulos en el diccionario que contiene todos los datos del dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e1e1bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "nulls = df.isna().mean().sort_values(ascending=False).mul(100).round(2)\n",
    "print(nulls)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f103adf7",
   "metadata": {},
   "source": [
    "En nuestro caso, no hay datos nulos, por lo tanto, no se deben borrar filas por este motivo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c8ce3d",
   "metadata": {},
   "source": [
    "6.3. Nos interesa saber la cardinalidad de cada columna, y tambien ver cuales son los valores distintos de ciertas columnas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1900c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cardinalidad = df.nunique(dropna=True).sort_values(ascending=False)\n",
    "print(cardinalidad)\n",
    "print(df[\"nombre_entidad\"].value_counts(dropna=False).head(10))\n",
    "print(df[\"nombre_fondo\"].value_counts(dropna=False).head(20))\n",
    "\n",
    "print(df.groupby(\"codigo_entidad\")[\"nombre_entidad\"].nunique().sort_values(ascending=False).head())\n",
    "print(df.groupby(\"codigo_patrimonio\")[\"nombre_fondo\"].nunique().sort_values(ascending=False).head())\n",
    "\n",
    "print(df.groupby(\"nombre_entidad\")[\"codigo_entidad\"].nunique().sort_values(ascending=False).head())\n",
    "print(df.groupby(\"nombre_fondo\")[\"codigo_patrimonio\"].nunique().sort_values(ascending=False).head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e96ac04",
   "metadata": {},
   "source": [
    "Con esto nos damos cuenta de que las columnas codigo_entidad y nombre_entidad estan relacionadas, y tambien las columnas codigo_patrimonio y nombre_fondo, a traves de una correspondencia uno a uno, lo cual significa que sera util remover dos columnas, codigo_entidad y codigo_patrimonio, para trabajar con valores relevantes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65172b9d",
   "metadata": {},
   "source": [
    "6.5. Luego vamos a crear un diccionario con key el nombre_entidad y value el codigo_entidad y otro diccionario con key el nombre_fondo y value codigo_patrimonio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37948999",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_entidad = (\n",
    "    df[[\"nombre_entidad\", \"codigo_entidad\"]]\n",
    "    .drop_duplicates()\n",
    "    .set_index(\"nombre_entidad\")[\"codigo_entidad\"]\n",
    "    .to_dict()\n",
    ")\n",
    "\n",
    "# Diccionario fondo (nombre → código)\n",
    "dict_fondo = (\n",
    "    df[[\"nombre_fondo\", \"codigo_patrimonio\"]]\n",
    "    .drop_duplicates()\n",
    "    .set_index(\"nombre_fondo\")[\"codigo_patrimonio\"]\n",
    "    .to_dict()\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1231e527",
   "metadata": {},
   "source": [
    "para luego crear un archivo csv con el contenido de los diccionarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af1a647b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_entidad = df[[\"nombre_entidad\", \"codigo_entidad\"]].drop_duplicates().set_index(\"nombre_entidad\")[\"codigo_entidad\"].to_dict()\n",
    "dict_fondo = df[[\"nombre_fondo\", \"codigo_patrimonio\"]].drop_duplicates().set_index(\"nombre_fondo\")[\"codigo_patrimonio\"].to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eabe1dfa",
   "metadata": {},
   "source": [
    "## 7. Identificación de columnas irrelevantes\n",
    "Exploramos valores únicos en `nombre_entidad` y `nombre_fondo` para \n",
    "decidir qué columnas conservar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ef4680",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Valores únicos en nombre_entidad:\", df[\"nombre_entidad\"].unique())\n",
    "print(\"Valores únicos en nombre_fondo:\", df[\"nombre_fondo\"].unique()[:10])\n",
    "\n",
    "print(df[\"nombre_entidad\"].value_counts())\n",
    "print(df[\"nombre_fondo\"].value_counts().head(20))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c555b0",
   "metadata": {},
   "source": [
    "## 8. Eliminamos las columnas irrelevantes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b110622",
   "metadata": {},
   "source": [
    "Eliminamos las columnas nombre_entidad y nombre_fondo, ya que son irrelevantes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b86e1d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_limpio = df.drop(columns=[\"codigo_entidad\", \"codigo_patrimonio\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5565dd64",
   "metadata": {},
   "source": [
    "y extraemos en un archivo csv los elementos de este DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aeda0c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_limpio.to_csv(\"data/raw/colfondosLimpio.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df1ef851",
   "metadata": {},
   "source": [
    "Ahora, vamos a limpiar las columnas nombre_entidad y nombre_fondo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a12a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in [\"nombre_entidad\", \"nombre_fondo\"]:\n",
    "    df[c] = (df[c]\n",
    "             .astype(str)\n",
    "             .str.strip()\n",
    "             .str.replace(r\"\\s+\", \" \", regex=True))  \n",
    "print(df[[\"nombre_entidad\",\"nombre_fondo\"]].nunique())\n",
    "\n",
    "print(\"Valores únicos en nombre_entidad:\", df[\"nombre_entidad\"].unique())\n",
    "print(\"Valores únicos en nombre_fondo:\", df[\"nombre_fondo\"].unique()[:10])\n",
    "\n",
    "print(df[\"nombre_entidad\"].value_counts())\n",
    "print(df[\"nombre_fondo\"].value_counts().head(20))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ac9b65",
   "metadata": {},
   "source": [
    "## 9. Crear nuevos datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb526fd",
   "metadata": {},
   "source": [
    "Al tener un dataset limpio, y al saber que hay pocas entidades y fondos, podemos crear nuevos datasets por cada entidad y fondo, para luego comparar los resultados y analizar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13db3d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "ath(\"data/raw\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def guardar_subset(df, col_filtro, valores, salida):\n",
    "    if isinstance(valores, (list, tuple, set)):\n",
    "        sub = df.loc[df[col_filtro].isin(valores)].copy()\n",
    "    else:\n",
    "        sub = df.loc[df[col_filtro].eq(valores)].copy()\n",
    "    if col_filtro in sub.columns:\n",
    "        sub = sub.drop(columns=[col_filtro])  \n",
    "    print(sub.shape)\n",
    "    sub.to_csv(salida, index=False)\n",
    "\n",
    "guardar_subset(df_limpio, \"nombre_entidad\",\n",
    "               \"Skandia Afp - Accai S.A.\",\n",
    "               \"data/raw/pensiones_skandia.csv\")\n",
    "\n",
    "guardar_subset(df_limpio, \"nombre_entidad\",\n",
    "               '\"Proteccion\"',\n",
    "               \"data/raw/pensiones_proteccion.csv\")\n",
    "\n",
    "guardar_subset(df_limpio, \"nombre_entidad\",\n",
    "               '\"Porvenir\"',\n",
    "               \"data/raw/pensiones_porvenir.csv\")\n",
    "\n",
    "guardar_subset(df_limpio, \"nombre_entidad\",\n",
    "               '\"Colfondos S.A.\" Y \"Colfondos\"',\n",
    "               \"data/raw/colfondos_colfondos.csv\")\n",
    "\n",
    "guardar_subset(df_limpio, \"nombre_fondo\",\n",
    "               \"Fondo de Cesantias Largo Plazo\",\n",
    "               \"data/raw/fondo_cesantias_largo_plazo.csv\")\n",
    "\n",
    "guardar_subset(df_limpio, \"nombre_fondo\",\n",
    "               \"Fondo de Cesantias Corto Plazo\",\n",
    "               \"data/raw/fondo_cesantias_corto_plazo.csv\")\n",
    "\n",
    "guardar_subset(df_limpio, \"nombre_fondo\",\n",
    "               \"Fondo de Pensiones Moderado\",\n",
    "               \"data/raw/fondo_pensiones_moderado.csv\")\n",
    "\n",
    "guardar_subset(df_limpio, \"nombre_fondo\",\n",
    "               \"Fondo de Pensiones Conservador\",\n",
    "               \"data/raw/fondo_pensiones_conservador.csv\")\n",
    "\n",
    "guardar_subset(df_limpio, \"nombre_fondo\",\n",
    "               \"Fondo de Pensiones Mayor Riesgo\",\n",
    "               \"data/raw/fondo_pensiones_mayor_riesgo.csv\")\n",
    "\n",
    "guardar_subset(df_limpio, \"nombre_fondo\",\n",
    "               \"Fondo de Pensiones Retiro Programado\",\n",
    "               \"data/raw/fondo_pensiones_retiro_programado.csv\")\n",
    "\n",
    "guardar_subset(df_limpio, \"nombre_fondo\",\n",
    "               \"Fondo de Pensiones Alternativo\",\n",
    "               \"data/raw/fondo_pensiones_alternativo.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32889e36",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7568faeb",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
